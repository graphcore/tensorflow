# Copyright 2021 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =============================================================================
"""
IPU specific Keras Model subclass extensions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
"""
import copy
import types

from tensorflow.python.ipu.keras.extensions import keras_extension_base
from tensorflow.python.keras.engine import base_layer_utils
from tensorflow.python.keras.engine import functional
from tensorflow.python.keras.engine import input_layer
from tensorflow.python.keras.engine import training as training_module
from tensorflow.python.training.tracking import base as trackable
from tensorflow.python.util import deprecation
from tensorflow.python.util import nest
from tensorflow.python.util import deprecation


class ModelLayerPipelineStageAssignment:
  """A class to indicate at which pipeline stage a layer in a `Model` subclass
  should be executed.

  Keras Layers can be called multiple times in order to share weights between
  layers. Each of these calls produces Tensor output which can be executed in
  different pipeline stages (as long as these stages are mapped to the same
  device).
  """
  @deprecation.deprecated(None, "Use `tf.keras` instead of `tf.python.keras`.")
  def __init__(self, layer, node_index, pipeline_stage=None):
    """Create a new `ModelLayerPipelineStageAssignment`.

    Args:
      layer (keras.layers.Layer): The Keras layer for which this assignment
        applies.
      node_index (int): The specific call to the `layer` that produced a Tensor.
        Layers can be called multiple times in order to share weights. A new
        `ModelLayerPipelineStageAssignment` is required for every Layer call.
        E.g. `node_index=0` will correspond to the first time the `layer` was
        called.
      pipeline_stage (int): If provided, indicates which pipeline stage this
        layer should be assigned to. If not provided this layer will be
        unassigned.
    """
    self._layer = layer
    self._node_index = node_index
    self.pipeline_stage = pipeline_stage

  @property
  def layer(self):
    """Returns the Keras layer associated with this assignment."""
    return self._layer

  @property
  def node_index(self):
    """Returns the specific call to the `layer` that produced a Tensor."""
    return self._node_index

  @property
  def inbound_layers(self):
    """Returns the input layers for the layer in this assignment. This can be
    useful for identifying which specific `node_index` this is."""
    node = self._layer._inbound_nodes[self.node_index]  # pylint: disable=protected-access
    return [n.layer for n in node.parent_nodes]

  @property
  def pipeline_stage(self):
    """Returns the pipeline stage this layer has been assigned to. If `None`,
    then this layer has not been assigned to a pipeline stage."""
    return self._pipeline_stage

  @pipeline_stage.setter
  def pipeline_stage(self, value):
    """Setter of `pipeline_stage` property. See `pipeline_stage` property
    doc.

    Args:
      value (int): The pipeline stage to assign this layer to."""
    self._pipeline_stage = value

  def __str__(self):
    return ("Layer: {} (node index {}) is assigned to pipeline "
            "stage: {}".format(self.layer.name, self.node_index,
                               self.pipeline_stage))


class ModelExtension(keras_extension_base.KerasExtensionBase):  # pylint: disable=abstract-method
  @trackable.no_automatic_dependency_tracking
  @deprecation.deprecated(None, "Use `tf.keras` instead of `tf.python.keras`.")
  def __init__(self, *args, **kwargs):  # pylint: disable=unused-argument
    keras_extension_base.KerasExtensionBase.__init__(self)
    self._pipeline_stage_assignment = []
    self._pipeline_maximum_stage = None

    # Track whether the graph network has been initialized.
    self._graph_network_initialized = False

    # In order to support pipelining, we need to store the graph generated by
    # the call function, like in a Functional model.
    # Copy functions for building a graph network from Functional.
    def bind_method(method):
      setattr(self, method.__name__, types.MethodType(method, self))

    # pylint: disable=protected-access
    bind_method(functional.Functional._compute_tensor_usage_count)
    bind_method(functional.Functional._init_graph_network)
    bind_method(functional.Functional._set_output_names)
    bind_method(functional.Functional._insert_layers)
    bind_method(functional.Functional._validate_graph_inputs_and_outputs)
    # Needed as they are called by keras_extension_base for pipelining.
    bind_method(functional.Functional._flatten_to_reference_inputs)
    bind_method(functional.Functional._conform_to_reference_input)
    # pylint: enable=protected-access

  def _is_pipelined(self):
    assert self._graph_network_initialized, \
      "`Model` subclasses cannot determine if they have pipeline stage " \
      "assignments until they have been built."
    return bool(self._pipeline_stage_assignment)

  def _get_config_supported(self):
    # We need to override get_config to be able to presist IPU-specific state
    # through saving and loading, such as pipeline stage assignments and
    # gradient accumulation options.
    # If get_config is overridden, we raise an error on loading which explains
    # how to include our config options.
    return True

  def _get_config_delegate(self):
    # Get the KerasExtensionBase config.
    config = self._get_base_config()

    # Add the graph network to config if we are using pipelining.
    if self._graph_network_initialized:
      config['graph_network_initialized'] = True
      network_config = copy.deepcopy(functional.get_network_config(self))
      config.update(network_config)

    # Marker to ensure the user has not incorrectly overridden this method.
    config["get_config_has_been_called"] = True

    # Add pipeling stage assignments.
    layer_to_index = {}
    for i, layer in enumerate(self.layers):
      layer_to_index[str(id(layer))] = i
    config["pipeline_stage_assignment"] = [
        (layer_to_index[str(id(assignment.layer))], assignment.node_index,
         assignment.pipeline_stage)
        for assignment in self._pipeline_stage_assignment
    ]
    return config

  def _build_supported(self, input_shape):
    del input_shape
    return True

  @trackable.no_automatic_dependency_tracking
  def _build_delegate(self, input_shape):
    # If we are not in a call context, then build has been called manually.
    # This is not annotated with generic_utils.default. This is so the Keras
    # internals know that the build method has been overridden, and should be
    # called before the model is first executed.

    # Check the call function has been overridden.
    self._validate_call_function()

    def map_shapes(input_shape, f):
      # Applies f to all of the shapes in input_shape, and returns the outputs
      # nested in the same way as input_shapes.
      # Helps with complexity introduced by accepting both standard shapes, and
      # keras.Input tensors.
      if isinstance(input_shape, (list, tuple)):
        if all(d is None or isinstance(d, int) for d in input_shape):
          return f(input_shape)
        return [f(shape) for shape in input_shape]
      if isinstance(input_shape, dict):
        return {k: f(shape) for k, shape in input_shape.items()}
      return f(input_shape)

    def get_shape(x):
      if hasattr(x, '_keras_history') \
            and isinstance(x._keras_history[0], input_layer.InputLayer):  # pylint: disable=protected-access
        # If input is a keras.Input tensor, extract the shape.
        return tuple(x.shape.as_list())
      # Otherwise, return the shape as-is.
      return x

    raw_shapes = map_shapes(input_shape, get_shape)
    # Bypass build from Model class, as this overrides that behaviour, and
    # call stright into base_layer build instead.
    # Get raw shapes in case the inputs include keras.Input tensors, which
    # aren't usually valid inputs for the build function. We allow them to
    # allow dtype information to be specified for tracing the graph.
    super(training_module.Model, self).build(raw_shapes)  # pylint: disable=bad-super-call

    call_context = base_layer_utils.call_context()
    if not call_context.in_call:
      # Only update the internal graph if we are not in a call context.
      # We only want to update if build has been manually called.
      def to_input_tensor(x):
        # Generate keras.Input tensors matching the specified input shape.
        # This allows us to trace the graph generated by call which is required
        # for pipelining.
        if hasattr(x, '_keras_history') \
            and isinstance(x._keras_history[0], input_layer.InputLayer):  # pylint: disable=protected-access
          # If it is already a keras.Input tensor, return it as-is.
          return x
        return input_layer.Input(batch_shape=x)

      self._check_call_args('build')
      inputs = map_shapes(input_shape, to_input_tensor)
      outputs = self._trace_graph_network(inputs)
      self._update_graph_network(inputs, outputs)

  @trackable.no_automatic_dependency_tracking
  def _create_input_with_dtypes(self, input_shape, input_dtype):
    if isinstance(input_shape, (list, tuple)):
      if all(d is None or isinstance(d, int) for d in input_shape):
        inputs = input_layer.Input(batch_shape=input_shape, dtype=input_dtype)
      else:
        assert isinstance(input_dtype, (list, tuple))
        assert len(input_dtype) == len(input_shape)
        inputs = [
            input_layer.Input(batch_shape=shape, dtype=dtype)
            for shape, dtype in zip(input_shape, input_dtype)
        ]
    elif isinstance(input_shape, dict):
      assert isinstance(input_dtype, dict)
      assert set(input_shape.keys()) == set(input_dtype.keys())
      # When using a dict input, self.inputs (a list) is ordered by key.
      # We want to match this ordering so we can compare the shapes and dtypes.
      sorted_keys = sorted(input_shape.keys())
      inputs = {
          k: input_layer.Input(batch_shape=input_shape[k],
                               dtype=input_dtype[k])
          for k in sorted_keys
      }
    else:
      inputs = input_layer.Input(batch_shape=input_shape, dtype=input_dtype)

    return inputs

  @trackable.no_automatic_dependency_tracking
  def _build_with_dtypes(self, input_shape, input_dtype):
    # Trace the graph with the given input shapes and dtypes, and update the
    # graph network.

    # This function is only called internally in case build has never been
    # mnually called. We don't want to overwrite the graph network
    # otherwise. If regenerating the graph network is required we leave that
    # up to the user.
    if self._graph_network_initialized:
      return

    # Check the call function has been overridden.
    self._validate_call_function()

    # Generate the keras.Input tensors needed to trace the call function.
    inputs = self._create_input_with_dtypes(input_shape, input_dtype)

    outputs = self._trace_graph_network(inputs)
    self._update_graph_network(inputs, outputs)

  def _verify_get_config_override(self, config):
    if self.get_config.__func__ != training_module.Model.get_config and \
        not config.get("get_config_has_been_called", False):
      raise RuntimeError(
          "get_config() has been overridden on a subclassed Model but "
          "IPU-specific properties have not been saved since the "
          "implementation does not call super().get_config(). "
          "In the get_config() override, add `config = super().get_config()` "
          "and merge your configuration with this config dict to ensure these "
          "options are saved along with the rest of your model.")

  def _deserialize_from_config_supported(self, config):
    del config
    return True

  def _deserialize_from_config_delegate(self, config):
    self._from_base_config(config)  # pylint: disable=protected-access
    self._verify_get_config_override(config)

    if config.get("graph_network_initialized", False):
      # Restore graph network if it's in the config.

      for layer in self.layers:
        # Clear nodes from all layers (except input layers) as restore_from_config
        # creates new nodes on top of the old ones, rather than reusing the old
        # ones. We need the node indices to be consistant for the pipelineÂ stage
        # assignments to be valid.
        if not isinstance(layer, input_layer.InputLayer):
          layer._inbound_nodes.clear()  # pylint: disable=protected-access
          layer._outbound_nodes.clear()  # pylint: disable=protected-access

      inputs, outputs, created_layers = \
          functional.reconstruct_from_config(config)
      self._update_graph_network(inputs, outputs)
      # Ancillery layers are layers which are not connected to the outputs,
      # such as layers added using `add_loss`.
      # This call restores those layers.
      functional.connect_ancillary_layers(self, created_layers)

      # Extract pipelining options.
      pipeline_stage_assignment = [
          ModelLayerPipelineStageAssignment(self.layers[layer_idx], node_index,
                                            stage)
          for layer_idx, node_index, stage in config.get(
              "pipeline_stage_assignment", [])
      ]
      if pipeline_stage_assignment:
        self.set_pipeline_stage_assignment(pipeline_stage_assignment)

  def set_asynchronous_callbacks(self, asynchronous=False):
    """Sets the asynchronous callbacks options when calling `fit()`, `evaluate()`
    and `predict()`.

    When running `fit()`, `evaluate()` and `predict()` the callbacks the model
    is configured with are executed after `steps_per_execution` have executed.
    Enabling asynchronous callbacks means that the callbacks are invoked after
    every step, even when `steps_per_execution > 1`. This can reduce the latency
    of receiving per step results and metrics at a cost of an extra thread
    running in the background of the application.
    Note that this option is ignored for the `fit()` and `evaluate()` when
    running a pipelined model and `accumulate_outfeed=True` (configured via
    `set_pipelining_options`).

    Args:
      asynchronous: Whether asynchronous callbacks should be enabled.
    """
    self._set_asynchronous_callbacks_impl(asynchronous)

  @deprecation.deprecated_args(
      None, '`experimental_normalize_gradients=True` has been '
      'deprecated and will be replaced in a future release with '
      'the use of mean reduction when accumulating gradients. '
      'Please update your optimizer settings.',
      'experimental_normalize_gradients')
  def set_gradient_accumulation_options(
      self,
      gradient_accumulation_steps_per_replica=None,
      experimental_normalize_gradients=None,
      gradient_accumulation_reduction_method='sum',
      **gradient_accumulation_optimizer_kwargs):
    # pylint:disable=line-too-long
    """Sets the gradient accumulation options for non-pipelined models which are
    to be used when training a model.

    When set, and `gradient_accumulation_steps_per_replica > 1`, the optimizer
    which the current model has been compiled with is wrapped in
    :class:`~tensorflow.python.ipu.optimizers.GradientAccumulationOptimizerV2`.
    This means that each replica will accumulate the gradients for
    `gradient_accumulation_steps_per_replica` steps, these accumulated gradients
    are then all-reduced across the replicas and the weight update is performed.

    Gradient Accumulation allows us to simulate bigger batch sizes. For example
    if we have a model where each step is of batch size 16 and we set
    `gradient_accumulation_steps_per_replica=4` and there is single replica in
    the system, this simulates an input batch of size 64.
    If we have a model where each step is of batch size 16 and we set
    `gradient_accumulation_steps_per_replica=4` and there are 4 replicas in
    the system, this simulates an input batch of size 256.

    See the :ref:`gradient-accumulation` section in the documention for more
    details.

    The value of `gradient_accumulation_steps_per_replica` has no effect when
    using `evaluate()` or `predict()`.

    Args:
      gradient_accumulation_steps_per_replica: An integer which indicates the
        number of steps the gradients will be accumulated for in each replica.
        This value multiplied by the number of replicas needs to divide the
        `steps_per_execution` value the model has been compiled with. This value
        is saved/loaded when the model is saved/loaded.
      experimental_normalize_gradients: If set to `True`, the gradients for each
        step are first scaled by
        `1/(gradient_accumulation_steps_per_replica * number of replicas)`
        before being added to the gradient accumulation buffer. Note that this
        option is experimental and the behavior might change in future releases.
        This value is saved/loaded when the model is saved/loaded.
      reduction_method: Reduction method to use when accumulating gradients.
        During the iterations in each optimizer step, the computed gradients
        can either be directly summed up or scaled such that we compute a mean
        of all gradients for each variable. Computing a mean avoids potential
        issues with overflow during accumulation especially when using
        float16, but gives smaller gradients and might require adjusting
        the learning-rate accordingly.
        Defaults to `GradientAccumulationReductionMethod.SUM`
        (see :class:`~tensorflow.python.ipu.optimizers.GradientAccumulationReductionMethod`)  # pylint: disable=line-too-long
      gradient_accumulation_optimizer_kwargs: All remaining keyword arguments
        are forwarded to
        :class:`~tensorflow.python.ipu.optimizers.GradientAccumulationOptimizerV2`.
        See the optimizer for all the available arguments. Must not contain
        `opt` or `num_mini_batches` as keys. Note that this dictionary is not
        serializable, which means that when the model is being saved, these
        values are not saved. When restoring/loading a model, please call
        `set_gradient_accumulation_options` again.
    """
    # pylint:enable=line-too-long
    self._set_gradient_accumulation_options_impl(
        gradient_accumulation_steps_per_replica,
        experimental_normalize_gradients,
        gradient_accumulation_reduction_method,
        gradient_accumulation_optimizer_kwargs)

  @deprecation.deprecated_args(
      None, '`experimental_normalize_gradients=True` has been '
      'deprecated and will be replaced in a future release with '
      'the use of mean reduction when accumulating gradients. '
      'Please update your pipeline settings.',
      'experimental_normalize_gradients')
  def set_pipelining_options(
      self,  #pylint: disable=missing-type-doc
      gradient_accumulation_steps_per_replica=None,
      device_mapping=None,
      accumulate_outfeed=None,
      experimental_normalize_gradients=None,
      gradient_accumulation_reduction_method='sum',
      **pipelining_kwargs):
    """Sets the pipelining options, including gradient accumulation options,
    for pipelined models.

    Before training a pipelined model, `gradient_accumulation_steps_per_replica`
    argument needs to be set as pipelined models always perform gradient
    accumulation when training. Setting
    `gradient_accumulation_steps_per_replica > 1` means that each replica will
    accumulate the gradients for `gradient_accumulation_steps_per_replica`
    steps, these accumulated gradients are then all-reduced across the replicas
    and the weight update is performed.

    Gradient Accumulation allows us to simulate bigger batch sizes. For example
    if we have a model where each step is of batch size 16 and we set
    `gradient_accumulation_steps_per_replica=4` and there is single replica in
    the system, this simulates an input batch of size 64.
    If we have a model where each step is of batch size 16 and we set
    `gradient_accumulation_steps_per_replica=4` and there are 4 replicas in
    the system, this simulates an input batch of size 256.

    When training a data-parallel model, enabling gradient accumulation also
    reduces the communication overhead as the all-reduce of gradients is now
    performed after each replica has performed
    `gradient_accumulation_steps_per_replica` steps instead of after each step.

    See the :ref:`gradient-accumulation` section in the documention for more
    details.

    The value of `gradient_accumulation_steps_per_replica` has no effect when
    using `evaluate()` or `predict()`.

    Args:
      gradient_accumulation_steps_per_replica: An integer which indicates the
        number of steps the gradients will be accumulated for in each replica.
        This value multiplied by the number of replicas needs to divide the
        `steps_per_execution` value the model has been compiled with. This value
        is saved/loaded when the model is saved/loaded.
      device_mapping: If provided, a list of length equal to the number of
        pipeline stages assigned in this model. An element at index `i` in the
        list represents which IPU the `i`'th pipeline stage should reside on.
        This can be used to make sure computational stages which share Keras
        layers/`tf.Variable` objects are resident on the same IPU. This value is
        saved/loaded when the model is saved/loaded.
      accumulate_outfeed: The metrics from the model are normally enqueued as
        soon as they're available. If this option is True, the data will
        instead be accumulated when they're available and enqueued at the end of
        pipeline execution, reducing the amount of host <-> device
        communication. When used with training, the accumulated metrics are
        normalised by `gradient_accumulation_steps_per_replica`. When used with
        evaluation, the accumulated metrics are normalised by `steps_per_epoch`.
        This option is ignored when doing prediction. When using
        `accumulate_outfeed`, model callbacks will be called with the same data
        for the batches which the data was accumulated for. This value is
        saved/loaded when the model is saved/loaded.
      experimental_normalize_gradients: If set to `True`, the gradients for each
        step are first scaled by
        `1/(gradient_accumulation_steps_per_replica * number of replicas)`
        before being added to the gradient accumulation buffer. Note that this
        option is experimental and the behavior might change in future releases.
        This value is saved/loaded when the model is saved/loaded.
      gradient_accumulation_reduction_method:  (Experimental)  Reduction method
        to use when accumulating gradients. During the iterations in each
        optimizer step, the computed gradients can either be directly summed up
        or scaled such that we compute a mean of all gradients for each
        variable. Computing a mean avoids potential issues with overflow during
        accumulation especially when using float16, but gives smaller gradients
        and might require adjusting the learning-rate accordingly.
        Defaults to `GradientAccumulationReductionMethod.SUM`
        (see :class:`~tensorflow.python.ipu.optimizers.GradientAccumulationReductionMethod`)  # pylint: disable=line-too-long
      pipelining_kwargs: All remaining keyword arguments are forwarded to
        :func:`~tensorflow.python.ipu.pipelining_ops.pipeline`. Note that this
        dictionary is not serializable, which means that when the model is
        being saved, these values are not saved. When restoring/loading a model,
        please call `set_pipelining_options` again.
    """
    self._set_pipelining_options_impl(gradient_accumulation_steps_per_replica,
                                      device_mapping, accumulate_outfeed,
                                      experimental_normalize_gradients,
                                      gradient_accumulation_reduction_method,
                                      pipelining_kwargs)

  def set_infeed_queue_options(self, **kwargs):
    """Sets the options for all instances of `IPUInfeedQueue` generated
    when executing the model.

    When using `fit()`, `evalute()` and `predict()`, an instance of
    :class:`~tensorflow.python.ipu.ipu_infeed_queue.IPUInfeedQueue` is created
    to efficiently feed data from the dataset to the device. Instances of
    `IPUInfeedQueue` can be created with optional arguments, such as
    `prefetch_depth`, which can increase the throughput of the model.

    Args:
      **kwargs: All keyword arguments are forwarded to
        :class:`~tensorflow.python.ipu.ipu_infeed_queue.IPUInfeedQueue`.
    """
    self._set_infeed_queue_options_impl(**kwargs)

  def set_outfeed_queue_options(self, **kwargs):
    """Sets the options for all instances of `IPUOutfeedQueue` generated
    when executing the model.

    When using `fit()`, `evalute()` and `predict()`, an instance of
    :class:`~tensorflow.python.ipu.ipu_infeed_queue.IPUOutfeedQueue` is created
    to efficiently feed data from the device to the host. Instances of
    `IPUOutfeedQueue` can be created with optional arguments, such as
    `buffer_depth`, which can increase the throughput of the model.

    Args:
      **kwargs: All keyword arguments are forwarded to
        :class:`~tensorflow.python.ipu.ipu_outfeed_queue.IPUOutfeedQueue`.
    """
    self._set_outfeed_queue_options_impl(**kwargs)

  def _trace_graph_network(self, inputs):
    # Clear the remembered nodes from any previous runs.
    # This is important to keep the node indices in the pipline stage assignment
    # consistant. Otherwise they will be invalidated by subsequent calls, even
    # if the graph structure was identical.
    for layer in self.layers:
      layer._inbound_nodes.clear()  # pylint: disable=protected-access
      layer._outbound_nodes.clear()  # pylint: disable=protected-access

    # Case where `training` is a positional arg with no default.
    kwargs = {}
    # Update self._call_full_argspec.
    self._init_call_fn_args()
    call_signature = self._call_full_argspec
    if len(call_signature.args) > 2:
      n_required = len(call_signature.args) - len(call_signature.defaults
                                                  or [])
      for arg in call_signature.args[2:n_required]:
        if arg == 'training':
          kwargs['training'] = False

    # Invoke call within a call_context in case metrics are added in call.
    # We can't invoke self() because the base_layer __call__ override sees that
    # we are passing in keras tensors and treats the whole model as a layer
    # being traced within a functional model.
    call_context = base_layer_utils.call_context()
    with call_context.enter(layer=self,
                            inputs=inputs,
                            build_graph=True,
                            training=None):
      if not self.built:
        # Calculate input_shape build in case it does setup required for call.
        def get_shape(tensor):
          return tuple(tensor.shape.as_list())

        if isinstance(inputs, (tuple, list, dict)):
          input_shape = nest.map_structure(get_shape, inputs)
        else:
          input_shape = get_shape(inputs)

        self.build(input_shape)
      return self.call(inputs, **kwargs)

  @trackable.no_automatic_dependency_tracking
  def _update_graph_network(self, inputs, outputs):
    # Call functional model code to setup graph network.
    self._init_graph_network(inputs, outputs)

    # _init_graph_network sets flags indicating which kwargs we expect to all
    # True. Calling this sets them back to being based on the call signature.
    self._init_call_fn_args()

    self._graph_network_initialized = True

    # Unset this flag which otherwise marks the model as a functional model.
    # Having this set can cause custom models to be incorrectly restored as
    # functional models, and it prevents input layers from being generated when
    # build is called.
    self._is_graph_network = False

    # Scan for pipeline stages applied with PipelineStage scope.
    # Do not overwrite existing assignments.
    if not self._pipeline_stage_assignment:
      post_order = self._create_post_order()

      def node_has_pipeline_stage(node):
        return (hasattr(node, "_pipeline_stage")
                or hasattr(node.outbound_layer, "_pipeline_stage"))

      # Input tensors don't get a pipeline stage so skip them.
      any_node_has_pipeline_stage = any(
          node_has_pipeline_stage(node)
          for node in post_order[len(self.inputs):])

      if not any_node_has_pipeline_stage:
        return

      # If any node has pipelining attached to it, then they all need it.
      # Create pipeline stage assignments.
      pipeline_stage_assignment = []
      for node in post_order[len(self.inputs):]:
        if not hasattr(node, "_pipeline_stage"):
          if not hasattr(node.outbound_layer, "_pipeline_stage"):
            raise ValueError(
                f"All layers of a pipelined model must have an associated "
                f"pipeline stage. However, {node.outbound_layer.name} has not "
                f"been assigned to one. Pipeline stages can be assigned when a "
                f"layer is constructed, or each time a layer is called. "
                f"Different pipeline stages can assigned to each call.")
          node._pipeline_stage = node.outbound_layer._pipeline_stage  # pylint: disable=protected-access
        layer = node.layer
        node_index = layer._inbound_nodes.index(node)  # pylint: disable=protected-access
        pipeline_stage_assignment.append(
            ModelLayerPipelineStageAssignment(
                layer, node_index, pipeline_stage=node._pipeline_stage))  # pylint: disable=protected-access

      self.set_pipeline_stage_assignment(pipeline_stage_assignment)

  def _get_pipelined_post_order(self, pipeline_stage_assignment):
    # Create a post order per pipeline stage as post order does not take
    # pipeline stages into account, for example multiple pipeline stages might
    # have output layers. Try reordering the nodes to preserve post order
    # and to make sure pipeline stages can still be executed in order.

    post_order_per_stage = {}
    num_inputs = len(self.inputs)
    post_order = self._create_post_order()

    if not isinstance(pipeline_stage_assignment, list):
      raise ValueError("`pipeline_stage_assignment` needs to be a list")

    if len(pipeline_stage_assignment) != (len(post_order) - num_inputs):
      raise ValueError(
          f"The length of the provided `pipeline_stage_assignment` "
          f"({len(pipeline_stage_assignment)}) does not match the number of "
          f"layers in the graph ({len(post_order) - num_inputs}). "
          f"Each layer needs to be assigned a pipeline stage "
          f"(excluding input layers).")

    # Assign inputs to stage 0.
    for node in post_order[:num_inputs]:
      layer = node._keras_history.layer  # pylint: disable=protected-access
      assert len(layer.inbound_nodes) == 1
      post_order_per_stage.setdefault(0, []).append(layer.inbound_nodes[0])

    stages = set()
    for assignment, node in zip(pipeline_stage_assignment,
                                post_order[num_inputs:]):
      if not isinstance(assignment, ModelLayerPipelineStageAssignment):
        raise ValueError(
            "All elements of `pipeline_stage_assignment` must be instances "
            "of `ModelLayerPipelineStageAssignment`.")

      if assignment.pipeline_stage is None:
        raise ValueError(
            f"Layer {assignment.layer.name} with node_index "
            f"{assignment.node_index} has not been assigned a pipeline stage "
            f"in `pipeline_stage_assignment`.")
      # Check that the assignment is for this node.
      if id(assignment.layer) != id(node.layer):
        raise ValueError(
            f"The order of `pipeline_stage_assignment` does not match the "
            f"post-order generated from the graph ({assignment.layer.name} "
            f"!= {node.layer.name}).")

      if not hasattr(assignment.layer, "_inbound_nodes"):
        raise ValueError(
            f"Layer {assignment.layer.name} has no recorded nodes in the "  # pylint: disable=protected-access
            f"graph, but `pipeline_stage_assignment` contains an assignment "
            f"for node_index {assignment.node_index}.")

      if len(assignment.layer._inbound_nodes) <= assignment.node_index:  # pylint: disable=protected-access
        raise ValueError(
            f"Layer {assignment.layer.name} has "  # pylint: disable=protected-access
            f"{len(assignment.layer._inbound_nodes)} invocation(s) in the "
            f"graph, but `pipeline_stage_assignment` contains an assignment "
            f"for node_index {assignment.node_index}.")

      assignment_node = assignment.layer._inbound_nodes[  # pylint: disable=protected-access
          assignment.node_index]
      if assignment_node != node:
        raise ValueError(
            f"The order of `pipeline_stage_assignment` does not match the "
            f"post-order generated from the graph (missmatched nodes "
            f"of layer {assignment.layer.name}).")
      # Add node to the post order for its pipeline stage.
      post_order_per_stage.setdefault(assignment.pipeline_stage,
                                      []).append(node)
      stages.add(assignment.pipeline_stage)

    max_stage = max(stages)
    expected_stages = set(range(max_stage + 1))
    if stages != expected_stages:
      missing_stages = sorted(expected_stages - stages)
      raise ValueError(
          f"Pipeline stage assignments must start at 0 and be strictly "
          f"increasing. The highest assignment found in "
          f"`pipeline_stage_assignment` was stage {max_stage}, however the "
          f"preceeding stages {missing_stages} had no assignments.")

    new_post_order_node_execution = []
    computed_set = set()
    # New post order executes all the layers within a pipeline stage and it
    # makes sure that all the layer inputs have already executed.
    for stage_id in range(max_stage + 1):
      for node in post_order_per_stage[stage_id]:
        all_inputs_executed = all(x in computed_set
                                  for x in node.flat_input_ids)
        if not all_inputs_executed:
          raise ValueError(
              f"Layer {node.outbound_layer.name} in pipeline stage {stage_id} "
              f"has a dependency from a pipeline stage which has not yet "
              f"been executed. Layers can only use outputs from current or "
              f"previous pipeline stages.")
        new_post_order_node_execution.append(node)
        # Update computed_set.
        computed_set.update([x for x in node.flat_output_ids])

    return post_order_per_stage, new_post_order_node_execution

  @trackable.no_automatic_dependency_tracking
  def _get_pipeline_post_order(self):
    try:
      post_order_per_stage, _ = self._get_pipelined_post_order(
          self._pipeline_stage_assignment)
    except ValueError as e:
      raise RuntimeError(
          f"The pipeline stage assignments for model {self.name} are no longer "
          f"valid: \"{e}\" This can happen if the model is invoked with "
          f"shapes or dtypes which are incompatible with those from the "
          f"previous invocation, or those passed to `model.build`. Try "
          f"calling `model.build` and specifying a less restrictive shape. You "
          f"can specify dtypes to `model.build` by passing in `keras.Input` "
          f"objects instead of plain shapes.")
    return post_order_per_stage

  @trackable.no_automatic_dependency_tracking
  def get_pipeline_stage_assignment(self):
    """Returns the pipeline stage assignment of the layers in the model.

    If `set_pipeline_stage_assignment()` has been called before, then it returns
    a copy of the current assignment, otherwise returns a list of
    `ModelLayerPipelineStageAssignment` for each layer in the model in
    post order (which means that layers are returned in the order they are
    executed).
    """
    if not self._graph_network_initialized:
      raise RuntimeError(
          f"Cannot get pipeline stage assignments for model `{self.name}` "
          f"which has not been built yet. Please call `model.build()` first.")

    if self._pipeline_stage_assignment:
      return copy.copy(self._pipeline_stage_assignment)

    post_order = self._create_post_order()

    output = []
    # Input tensors don't get a pipeline stage so skip them.
    for node in post_order[len(self.inputs):]:
      layer = node.layer
      node_index = layer._inbound_nodes.index(node)  # pylint: disable=protected-access
      output.append(ModelLayerPipelineStageAssignment(layer, node_index))

    return output

  @trackable.no_automatic_dependency_tracking
  def _validate_pipeline_stage_assignment(self, pipeline_stage_assignment):
    self._get_pipelined_post_order(pipeline_stage_assignment)

  @trackable.no_automatic_dependency_tracking
  def set_pipeline_stage_assignment(self, pipeline_stage_assignment):
    """Sets the pipeline stage assignment for all the layers in the model.

    Sets the pipeline stage assignment of all the layers in the model which is
    used to create a model-parallel execution of this `Model` when calling
    `fit()`, `evaluate()` and `predict()`. Note that this pipelining stage
    assignment is ignored when using the `call()` function on this model.

    Args:
      pipeline_stage_assignment: A list of the same length as the number of
        layers in this model. All elements can be either intergers or instances
        of `ModelLayerPipelineStageAssignment`. If all the elements are
        integers, then a layer in this model at index `i` is assigned to a
        pipeline stage `pipeline_stage_assignment[i]`. Otherwise, if all the
        elements are of type `ModelLayerPipelineStageAssignment` then a
        layer in this model at index `i` is assigned to a pipeline stage
        indicated by `pipeline_stage_assignment[i].pipeline_stage`.

    Raises:
      ValueError: `pipeline_stage_assignment` is not a valid assignment.
    """
    if not self._graph_network_initialized:
      raise RuntimeError(
          f"Cannot set pipeline stage assignments for model `{self.name}` "
          f"which has not been built yet. Please call `model.build()` first, "
          f"or assign  pipeline stages in your graph definition using "
          f"PipelineStage scopes.")

    self._validate_pipeline_stage_assignment(pipeline_stage_assignment)
    self._pipeline_stage_assignment = pipeline_stage_assignment

    # Pipelining has changed therefore functions need to be recompiled.
    self._reset_ipu_extension()

  @trackable.no_automatic_dependency_tracking
  def reset_pipeline_stage_assignment(self):
    """Resets the pipeline stage assignment so that the model is no longer
    pipelined."""
    self._pipeline_stage_assignment = []
    self._pipeline_maximum_stage = None

    # Pipelining has changed therefore functions need to be recompiled.
    self._reset_ipu_extension()

  @trackable.no_automatic_dependency_tracking
  def print_pipeline_stage_assignment_summary(self,
                                              line_length=None,
                                              print_fn=None):
    """Prints a summary of the pipeline stage assignment of the model.

    Arguments:
        line_length: Total length of printed lines (e.g. set this to adapt the
          display to different terminal window sizes).
        print_fn: Print function to use. It will be called on each line of the
          summary. You can set it to a custom function in order to capture the
          string summary. It defaults to `print` (prints to stdout).
    """
    line_length = line_length or 89

    def print_assignment_fn(assignment, print_row):
      layer = assignment.layer
      node_index = str(assignment.node_index)
      inbound_layers = nest.flatten(assignment.inbound_layers)
      pipeline_stage = str(assignment.pipeline_stage)

      name = layer.name
      input_layer_names = [l.name for l in inbound_layers]

      cls_name = layer.__class__.__name__
      if not input_layer_names:
        first_input = ''
      else:
        first_input = input_layer_names[0]

      fields = [
          name + ' (' + cls_name + ') (' + node_index + ')', first_input,
          pipeline_stage
      ]
      print_row(fields)

      # Print other inputs on the new line.
      if len(input_layer_names) > 1:
        for i in range(1, len(input_layer_names)):
          fields = ['', input_layer_names[i], '']
          print_row(fields)

    headers = ['Layer (type) (node index)', 'Input Layers', 'Pipeline Stage']
    column_widths = [.4, .8, 1.]
    self._print_pipeline_stage_assignment_summary_impl(print_assignment_fn,
                                                       headers, column_widths,
                                                       line_length, print_fn)

  @trackable.no_automatic_dependency_tracking
  def _get_pipeline_maximum_pipeline_stage(self):
    assert self._is_pipelined()
    if self._pipeline_maximum_stage is None:
      self._pipeline_maximum_stage = self._pipeline_stage_assignment[
          -1].pipeline_stage
    return self._pipeline_maximum_stage

  def _validate_call_function(self):
    call_function_implemented = not (
        hasattr(self.call, "__func__")
        and self.call.__func__ == training_module.Model.call)
    if not call_function_implemented:
      raise NotImplementedError(
          f"The function `call` for the model {self.name} has not been "
          f"implemented. When subclassing the `Model` class you must implement "
          f"the `call` function.")
